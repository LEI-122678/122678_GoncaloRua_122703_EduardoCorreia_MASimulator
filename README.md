# MASimulator - Simulador de Sistemas Multi-Agente com Aprendizagem

Este projeto implementa uma infraestrutura de simulaÃ§Ã£o para estudar e comparar paradigmas de Agentes AutÃ³nomos (**Q-Learning** vs **NeuroevoluÃ§Ã£o/NEAT**) em ambientes de complexidade variÃ¡vel.

O foco principal Ã© a comparaÃ§Ã£o de performance entre a aprendizagem por reforÃ§o clÃ¡ssica e a evoluÃ§Ã£o de redes neuronais assistida por **Novelty Search** (Busca por Novidade) na resoluÃ§Ã£o de labirintos com mÃ­nimos locais enganadores.

**Unidade Curricular:** Agentes AutÃ³nomos (2025/2026) - ISCTE-IUL

## ðŸ‘¥ Autores
* **Eduardo David Rivas Correia** (nÂº 122703)
* **GonÃ§alo Costa Rua** (nÂº 122678)

## ðŸ“‚ Estrutura do Projeto

```text
MASimulator/
â”‚
â”œâ”€â”€ agentes/                   # LÃ³gica interna dos agentes
â”‚   â”œâ”€â”€ Accao.py
â”‚   â”œâ”€â”€ Agente.py
â”‚   â”œâ”€â”€ Observacao.py
â”‚   â”œâ”€â”€ Politicas.py
â”‚   â””â”€â”€ Sensor.py
â”‚
â”œâ”€â”€ ambiente/                  # DefiniÃ§Ã£o dos mundos e regras
â”‚   â”œâ”€â”€ AmbienteFarol.py
â”‚   â”œâ”€â”€ AmbienteMaze.py
â”‚   â””â”€â”€ Obstaculos.py
â”‚
â”œâ”€â”€ mazes/                     # Mapas dos labirintos (txt)
â”‚   â”œâ”€â”€ dificuldade1.txt
â”‚   â””â”€â”€ dificuldade2.txt
â”‚   â”œâ”€â”€ dificuldade3.txt
â”‚   â””â”€â”€ dificuldade4.txt
â”‚
â”œâ”€â”€ simulador/                 # Core da simulaÃ§Ã£o
â”‚   â”œâ”€â”€ MotorDeSimulacao.py
â”‚   â”œâ”€â”€ NoveltyArchive.py      # Algoritmo de Novelty Search
â”‚   â””â”€â”€ Visualizador.py
â”‚
â”œâ”€â”€ vencedores/                # Modelos treinados prontos a testar (.pkl)
â”‚   â”œâ”€â”€ vencedor_FAROL.pkl
â”‚   â”œâ”€â”€ vencedor_FAROLQL.pkl
â”‚   â”œâ”€â”€ vencedor_MAZE[1-4]     # Modelos para os 4 nÃ­veis de dificuldade
â”‚   â””â”€â”€ vencedor_PAREDES...    # CenÃ¡rios de teste extra
â”‚
â”œâ”€â”€ config-feedforward.txt     # ConfiguraÃ§Ã£o do algoritmo NEAT
â”œâ”€â”€ graficos.py                # Gera os grÃ¡ficos comparativos de performance
â”œâ”€â”€ main.py                    # Motor principal para visualizar a simulaÃ§Ã£o
â”œâ”€â”€ parametros.json            # ConfiguraÃ§Ãµes globais lidas pelo main.py
â”œâ”€â”€ treino_neat.py             # Script de treino (NEAT)
â””â”€â”€ treino_qlearning.py        # Script de treino (Q-Learning)
`````

## ðŸš€ Como Executar (Modo VisualizaÃ§Ã£o)

Para testar os agentes e visualizar a simulaÃ§Ã£o, deve configurar o ficheiro `parametros.json` e executar o `main.py`.

### Passo 1: Configurar o CenÃ¡rio
Abra o ficheiro `parametros.json` na raiz do projeto e edite os campos conforme o teste desejado. Abaixo estÃ£o exemplos de configuraÃ§Ã£o para os cenÃ¡rios mais importantes.

#### A. Testar PolÃ­tica NEAT no Labirinto Mais DifÃ­cil (Maze 4)

```json
{
  "ambiente": {
    "tipo": "maze",
    "dificuldade": 4,
    "_comentario": "NÃ£o necessÃ¡rio altura e largura pois o maze nao depende disso, mas sim de ficheiros .txt prÃ©-feitos"
  },
  "agentes": [
    {
      "id": 1,
      "posicao_inicial": [1, 1],
      "politica": {
        "tipo": "aprendizagem",
        "ficheiro": "vencedor_MAZE4.pkl"
      },
      "sensores": [
        { "direcao": [1, 0], "movimentos": 1 },
        { "direcao": [-1, 0], "movimentos": 1 },
        { "direcao": [0, 1], "movimentos": 1 },
        { "direcao": [0, -1], "movimentos": 1 }
      ]
    }
  ]
}
`````
#### B. Testar PolÃ­tica Q-Learning no Problema do Farol
```json
{
  "ambiente": {
    "tipo": "farol",
    "largura": 15,
    "altura": 10,
    "dificuldade": 2
  },
  "agentes": [
    {
      "id": 1,
      "posicao_inicial": [1, 1],
      "politica": {
        "tipo": "qlearning",
        "ficheiro": "vencedor_FAROLQL.pkl"
      },
      "sensores": [
        { "direcao": [1, 0], "movimentos": 1 },
        { "direcao": [-1, 0], "movimentos": 1 },
        { "direcao": [0, 1], "movimentos": 1 },
        { "direcao": [0, -1], "movimentos": 1 }
      ]
    }
  ]
}
`````
#### C. Testar PolÃ­tica Fixa no Problema do Faro
```json
{
  "ambiente": {
    "tipo": "farol",
    "largura": 15,
    "altura": 10,
    "dificuldade": 3
  },
  "agentes": [ 
      {
      "id": 3,
      "posicao_inicial": [1, 1],
      "politica": {
        "tipo": "fixa",
        "_comentario": "NÃ£o necessÃ¡rio ficheiro pois a polÃ­tica fixa nao necessita disso"
      },
      "sensores": [
        { "direcao": [1, 0], "movimentos": 1 },
        { "direcao": [-1, 0], "movimentos": 1 },
        { "direcao": [0, 1], "movimentos": 1 },
        { "direcao": [0, -1], "movimentos": 1 }
      ]
    }
  ]
}
`````


### Passo 2: Executar
ApÃ³s guardar as alteraÃ§Ãµes no ficheiro JSON, corra o comando:

```bash
python main.py
`````

## ðŸ§  Como Treinar (Modo Aprendizagem)

Se desejar treinar novos agentes de raiz em vez de usar os modelos prÃ©-treinados:

### Treino com Q-Learning
Abra o ficheiro treino_qlearning.py, ajuste a variÃ¡vel USAR_MAZE (True ou False), e DIFICULDADE = 1 a 4, conforme o ambiente desejado e execute:

```bash
python treino_qlearning.py
`````

### Treino com NEAT (Redes Neuronais)
Abra o ficheiro treino_neat.py, ajuste a configuraÃ§Ã£o USAR_MAZE e DIFICULDADE e execute:

```bash
python treino_neat.py
`````

O treino irÃ¡ gerar grÃ¡ficos de evoluÃ§Ã£o da fitness e guardarÃ¡ o melhor modelo na pasta vencedores/.


